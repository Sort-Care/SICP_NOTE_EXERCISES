* Building Abstractions with Data
#+begin_center
We now come to the decisive step of mathematical abstraction: we forget about what the symbols stand for.
...[The mathematician] need not be idle; there are many operations which he may carry out with these symbols,
without ever having to look at the things they stand for.
--- Hermann Weyl, The Mathematical Way of Thinking
#+end_center

We concentrated in chapter 1 on computational processes and on the role of procedures in program design.
We saw *how to use primitive data* (numbers) and *primitive operations* (arithmetic operations), *how to combine procedures to form compound procedures through composition*, *conditionals* and the *use of parameters*, 
and *how to abstract procedures by using* ~define~.

We saw that a procedure can be regarded as a pattern for the local evolution of a process, and we classified, reasoned about and performed simple algorithmic analyses of some common patterns for processes as embodied in procedures. 

*We also saw that higher-order procedures enhance the power of our language by enabling us to manipulate, and there by to reason in terms of, general methods of computation. This is much of the essence of programming.*

In this chapter we are going to look at more complex data. All the procedures in chapter 1 operate on simple numerical data, and simple data are not sufficient for many of the problems we wish to address using computation. Programs are typically designed to model complex phenomena, and more often than not one must construct computational objects that have several parts in order to model real-world phenomena that have several aspects. Thus, whereas our focus in chapter 1 was on building abstractions by combining procedures to form compound procedures, we turn in this chapter to another key aspect of any programming language: the mans it provides for building abstractions by combining data objects to form /compound data/.

The reason that we want compound data in a programming language: *to elevate the conceptual level at which we can design our programs*, *to increase the modularity of our designs*, and *to enhance the expressive power of our language*. 

Just as the ability to define procedures enables us to deal with processes at a higher-order conceptual level that of the primitive operations of the language, the ability to construct compound data objects enables us to deal with data at a higher conceptual level than that of the primitive data objects of the objects of the language.

Consider designing a system to perform arithmetic with rational numbers. We could imagine an operation ~add-rat~ that takes two rational numbers and produces their sum.

A rational number can be thought of two integers: a numerator and a denominator. Thus, designing a program that within which a rational number is represented by two integers and where ~add-rat~ is implemented by two procedures for adding up and creating the numerator and denominator of the sum respectively can solve the problem.However, using that approach means we need to always explicitly keep track of which numerators correspond to which denominators. In a system intended to perform many operations on many rational numbers, such bookkeeping details would clutter the programs substantially, to say nothing of what they would do to our minds. 

It would be much better if we could "glue together" a numerator and denominator to form a pair--a /compound data object/--that our programs could manipulate in a way that would be consistent with regarding a rational number as a single conceptual unit.

The use of compound data also enables us to increase the modularity of our programs. If we can manipulate rational numbers directly as objects in their own right, then we can separate the part of our program that deals with rational numbers per se from the details of how rational numbers may be represented as pairs of integers.

The general technique represented from the parts of a program that deal with how data objects are used is a powerful design methodology called /data abstraction/. We will see how data abstraction makes programs much easier to design, maintain, and modify.

The use of compound data leads to a real increase in the expressive power of our programming language. Consider the idea of forming a "linear combination" $ax+by$. We might like to write a procedure that would accept $a$, $b$, $x$, and $y$ as arguments and return the value of $ax+by$. This presents no difficulty if the arguments are to be numbers, because we can readily define the procedure
#+begin_src scheme
  (define (linear-combination a b x y)
    (+ (* a x) (* b y)))
#+end_src

But suppose we are not concerned only with numbers. Suppose we would like to express, in procedural terms, the idea that one can from linear combinations whenever addition and multiplication are defined--for rational numbers, complex numbers, polynomials, or what ever. We can express this as a procedure of the form
#+begin_src scheme
  (define (linear-combination a b x y)
    (add (mul a x) (mul b y)))
#+end_src

where ~add~ and ~mul~ are not the primitive procedures ~+~ and ~*~ but rather more complex things that will perform the appropriate operations for whatever kinds of data we pass in as the arguments ~a~, ~b~, ~x~, and ~y~. The key point is that the only thing ~linear-combination~ should need to know about ~a~, ~b~, ~x~, and ~y~ is that the procedures ~add~ and ~mul~ will perform the appropriate manipulations. From the perspective of the procedure ~linear-combination~, it is irrelevant what ~a~, ~b~, ~x~, and ~y~ are and even more irrelevant how they might happen to be represented in terms of more primitive data.

This same example shows why it is important that our programming language provide the ability to manipulate compound objects directly: Without this, there is no way for a procedure such as ~linear-combination~ to pass its arguments along to ~add~ and ~mul~ without having to know their detailed structure.

The ability to directly manipulate procedures provides an analogous increase in the expressive power of a programming language. For example, in section 1.3.1 we introduced the ~sum~ procedure, which takes a procedure ~term~ as an argument and computes the sum of the ~term~ over some specified interval. In order to define ~sum~, it is crucial that we be able to speak of a procedure such as ~term~ as an entity in its own right, without regard for how ~term~ might be expressed with more primitive operations.
Indeed, if we did not have the notion of a "procedure", it is doubtful that we would ever even think of the possibility of defining an operation such as ~sum~. Moreover, insofar as performing the summation is concerned, the details of how ~term~ may be constructed from more primitive operations are irrelevant.

We begin this chapter by implementing the rational-number arithmetic system mentioned above. This will form the background for our discussion of *compound data* and *data abstraction*.As with compound procedures, the main issue to be addressed is that of abstraction as a technique of coping with complexity, and we will see how data abstraction enables us to erect suitable *abstraction barriers* between different parts of a program.

We will see that the key to forming compound data is that *a programming language should provide some kind of "glue" so that data objects can be combined to form more complex data objects*.

There are many possible kinds of glue. Indeed, we will discover how to form compound data using no special "data" operations at all, only procedures. This will further blur the distinction between "procedure" and "data", which was already becoming tenuous toward the end of chapter 1. We will also explore some conventional techniques for representing sequences and trees.

One key idea in dealing with compound data is the notion of *closure*--that the glue we use for combining data objects should allow us to combine not only primitive data objects, but compound data objects as well.

Another key idea is that compound data objects can serve as /conventional interfaces/ for combining program modules in mix-and-match ways. We illustrate some of these ideas by presenting a simple graphics language that exploits closure.

We will then augment the representational power of our language by introducing /symbolic expressions/--data whose elementary pats can be arbitrary symbols rather than only numbers. We explore various alternatives for representing sets of objects. We will find that, just as a given processes, there are many ways in which a given data structure can be represented in terms of simpler objects, and the choice of representation can have significant impact on the time and space requirements of processes that manipulate the data. We will investigate these ideas in the context of symbolic differentiation, the representation of sets, and the encoding of information.

Next we will take up the problem of working with data that may be represented differently by different parts of a program. This leads to the need to implement *generic operations*, which must handle many different types of data. Maintaining modularity in the presence of generic operations requires more powerful abstraction barriers than can be erected with simple data abstraction alone. In particular, we introduce /data-directed programming/ as a technique that allows individual data representations to be designed in isolation and then combined /additively/ (i.e., without modification). To illustrate the power of this approach to system design, we close the chapter by applying what we have learned to the implementation of a package for performing symbolic arithmetic on polynomials, in which the coefficients of the polynomials can be integers, rational numbers, complex numbers, and even other polynomials.
** Introduction to Data Abstraction
In section 1.1.8, we noted that a procedure used as an element in creating a more complex procedure could be regarded not only as a collection of particular procedure could be regarded not only as a collection of particular operations but also as a procedural abstraction. That is, *the details of how the procedure was implemented could be suppressed, and the particular procedure itself could be replaced by any other procedure with the same overall behavior*. In other words, we could make an abstraction that would separate the way the procedure would be used from the details of how the procedure would be implemented in terms of more primitive procedures. The analogous notion for compound data is called *Data Abstraction*. Data abstraction is a methodology that enables us to isolate how a compound data object is used from details of how it is constructed from more primitive data objects.

The basic idea of data abstraction is to structure the programs that are to use compound data objects so that they operate on "abstract data." That is, our programs should use data in such a way as to make no assumptions about the data that are not strictly necessary for performing the task at hand. At the same time, a "concrete" data representation is defined independent of the programs that use the data. The interface between these two parts of our system will be a set of procedures, called /selectors/ and /constructors/, that implement the abstract data in terms of the concrete representation. To illustrate this technique, we will consider how to design a set of procedures for manipulating rational numbers.
*** Example: Arithmetic Operations for Rational Numbers
Suppose we want to do arithmetic with rational numbers. We want to be able to add, subtract, multiply, and divide them and to test whether two rational numbers are equal.

Let us begin by assuming that we already have a way of constructing a rational number from a numerator and a denominator. We also assume that, given a rational number, we have a way of extracting (or selecting) its numerator and its denominator. Let us further assume that the constructor and selectors are available as procedures:
- ~(make-rat <n> <d>)~ returns the rational number whose numerator is the integer ~<n>~ and whose denominator is the integer ~<d>~.
- ~(numer <x>)~ returns the numerator of the rational number ~<x>~
- ~(denom <x>)~ returns the denominator of the rational number ~<x>~.

We are using here a powerful strategy of synthesis: /wishful thinking/. We haven't yet said how a rational number is represented, or how the procedures ~numer~, ~denom~, ~make-rat~ should be implemented. Even so, if we did have these three procedures, we could then ~add~, ~subtract~, ~multiply~, ~divide~, and test equality by using the following relations:

$$
\begin{align*}
& \frac{n_1}{d_1} + \frac{n_2}{d_2} = \frac{n_1 d_2 + n_2 d_1}{d_1 d_2} \\
& \frac{n_1}{d_1} - \frac{n_2}{d_2} = \frac{n_1 d_2 - n_2 d_1}{d_1 d_2} \\
& \frac{n_1}{d_1} \cdot \frac{n_2}{d_2} = \frac{n_1 n_2}{d_1 d_2}\\
& \frac{n_1/d_1}{n_2/d_2} = \frac{n_1 d_2}{d_1 n_2}\\
& \frac{n_1}{d_1} = \frac{n_2}{d_2}\ \ \text{if and only if}\ n_1 d_2 = n_2 d_1
\end{align*}
$$

We can express these rules as procedures:
#+begin_src scheme
  (define (add-rat x y)
    (make-rat (+ (* (numer x) (denom y))
                 (* (numer y) (denom x)))
              (* (denom x)
                 (denom y))))

  (define (sub-rat x y)
    (make-rat (- (* (numer x) (denom y))
                 (* (numer y) (denom x)))
              (* (denom x)
                 (denom y))))

  (define (mul-rat x y)
    (make-rat (* (numer x) (numer y))
              (* (denom x) (denom y))))

  (define (div-rat x y)
    (make-rat (* (numer x) (denom y))
              (* (denom x) (numer y))))

  (define (equal-rat? x y)
    (= (* (numer x) (denom y))
       (* (numer y) (denom x))))
#+end_src

Now we have the operations on rational numbers defined in terms of the selector and constructor procedures ~numer~, ~denom~, and ~make-rat~. But we haven't yet defined these. What we need is some way to glue together a numerator and a denominator to form a rational number.
**** Pairs
To enable us to implement the concrete level of our data abstraction, our language provides a compound structure called a ~pair~, which can be constructed with the primitive procedure ~cons~. This procedure takes two arguments and returns a compound data object that contains the two arguments as parts. Given a pair, we can extract the parts using the primitive procedures ~car~ and ~cdr~. Thus, we can use ~cons~, ~car~, and ~cdr~ as follows:
#+begin_src scheme
  (define x (cons 1 2))

  (car x) ;; 1
  (cdr x) ;; 2
#+end_src

Notice that a pair is a data object that can be given a name and manipulated, just like a primitive data object. Moreover, ~cons~ can be used to form pairs whose elements are pairs, and so on:
#+begin_src scheme
  (define x (cons 1 2))
  (define y (cons 3 4))
  (define z (cons x y))

  (car (car z));; 1
  (car (cdr z));; 3
#+end_src
In section 2.2 we will see how this ability to combine pairs means that pairs can be used as general-purpose building blocks to create all sorts of complex data structures. The single compound-data primitive /pair/, implemented by the procedures ~cons~, ~car~, and ~cdr~ is the only glue we need. Data objects constructed from pairs are called /list-structured/ data.
**** Representing Rational Numbers
Pairs offer a natural way to complete the rational-number system. Simply represent a rational number as a pair of two integers: a numerator and a denominator. Then ~make-rat~, ~numer~, and ~denom~ are readily implemented as follows:
#+begin_src scheme
  (define (make-rat n d) (cons n d))
  (define (numer x) (car x))
  (define (denom x) (cdr x))
#+end_src

Also, in order to display the results of our computations, we can print rational numbers by printing the numerator, a slash, and the denominator.
#+begin_src scheme
  (define (print-rat x)
    (newline)
    (display (numer x))
    (display "/")
    (display (denom x)))
#+end_src

Now we can try our rational-number procedures:
#+begin_src scheme
  (define one-half (make-rat 1 2))
  (print-rat one-half)
  (define one-third (make-rat 1 3))
  (print-rat (add-rat one-half one-third))
  (print-rat (mul-rat one-half one-third))
  (print-rat (add-rat one-third one-third))
#+end_src

As the final example shows, our rational-number implementation does not reduce rational numbers to lowest terms. We can remedy this by changing ~make-rat~. If we have a ~gcd~ procedure like the one in section 1.2.5 that produces the greatest common divisor of two integers, we can use ~gcd~ to reduce the numerator and the denominator to lowest terms before constructing the pair:
#+begin_src scheme
  (define (make-rat n d)
    (let ((g (gcd n d)))
      (cons (/ n g) (/ d g))))
#+end_src
This modification was accomplished by changing the constructor ~make-rat~ without changing any of the procedures (such as ~add-rat~ and ~mul-rat~) that implement the actual operations.
*** Abstraction Barriers
Before continuing with more examples of compound data and data abstraction, let us consider some of the issues raised by the rational-number example. We define the rational-number operations in terms of a constructor ~make-rat~ and selectors ~numer~ and ~denom~. In general, the underlying idea of data abstraction is to identify for each type of data object a basic set of operations in terms of which all manipulations of data objects of that type will be expressed, and then to use only those operations in manipulating the data.

#+begin_verse
----------- Programs that use rational numbers -----------
Rational Numbers in Problem domain

----------- ~add-rat~ ~sub-rat~ ... ----------------------
Rational numbers as numerators and denominators

----------- ~make-rat~ ~numer~ ~denom~ -------------------
Rational numbers as pairs

----------- ~cons~ ~car~ ~cdr~ ---------------------------
However pairs are implemented
#+end_verse

We can envision the structure of the rational-number system as shown in figure 2.1. The horizontal lines represent /abstraction barriers/ that isolate different "levels" of the system. At each level, the barrier separates the programs (above) that use the data abstraction from the programs (below) that implement the data abstraction. Programs use rational numbers manipulate them solely in terms of the procedures supplied "for public use" by the rational-number package: ~add-rat~, ~sub-rat~, ~mul-rat~, ~div-rat~, and ~equal-rat?~. These, in turn, are implemented solely in terms of the constructor and the selectors ~nake-rat~, ~numer~, and ~denom~, which themselves are implemented in terms of pairs. The details of how pairs are implemented are irrelevant to the rest of the rational-number package so long as pairs can be manipulated by the use of ~cons~, ~car~, and ~cdr~. In effect, procedures at each level are the interfaces that define the abstraction barriers and connect the different levels.

This simple idea has many advantages. One advantage is that it makes programs much easier to maintain and to modify. Any complex data structure can be represented in a variety of ways with the primitive data structures provided by a programming language. Of course, the choice of representation influences the programs that operate on it; thus, if the representation were to be changed at some later time, all such programs might have to be modified accordingly. This task could be time-consuming and expensive in the case of large programs unless the dependence on the representation were to be confined by design to a very few program modules.

For example, an alternate way to address the problem of reducing rational numbers to lowest terms is to perform the reduction whenever we access the parts of a rational number, rather than when we construct it. This leads to different constructor and selector procedures:
#+begin_src scheme
  (define (make-rat n d)
    (cons n d))

  (define (numer x)
    (let ((g (gcd (car x) (cdr x))))
      (/ (car x) g)))

  (define (denom x)
    (let ((g (gcd (car x) (cdr x))))
      (/ (cdr x) g)))
#+end_src

The difference between this implementation and the previous once lies in when we compute the ~gcd~. If our typical use of rational numbers we access the numerators and denominators of the same rational numbers many times, it would be preferable to compute the ~gcd~ when the rational numbers are constructed. If not, we may be better off waiting until access time to compute the ~gcd~. In any case, when we change from one representation to the other, the procedures ~add-rat~, ~sub-rat~, and so on do not have to be modified at all.

Constraining the dependence on the representation to a few interface procedures helps us design programs as well as modify them, because it allows us to maintain the flexibility to consider alternate implementations. To continue with our simple example, suppose we are designing a rational-number package and we can't decide initially whether to perform the ~gcd~ at construction time or at selection time. The data abstraction methodology gives us a way to defer that decision without losing the ability to make progress on the rest of the system.
*** What is Meant by Data?
We began the rational-number implementation in section 2.1.1 by implementing the rational-number operations ~add-rat~, ~sub-rat~, and so on in terms of three unspecified procedures: ~make-rat~, ~numer~, and ~denom~.
At that point, we could think of the operations as being defined in terms of data object--numerators, denominators, and rational numbers--whose behavior was specified by the latter three procedures.

But exactly what is meant by /data/? It is not enough to say "whatever is implemented by the given selectors and constructors." Clearly, not every arbitrary set of three procedures can serve as an appropriate basis for the rational-number implementation. We need to guarantee that, if we construct a rational number ~x~ from a pair of integers $n$ and $d$, then extracting the ~numer~ and ~denom~ of $x$ and dividing them should yield the same result as dividing $n$ by $d$. In other words, ~make-rat~, ~numer~, and ~denom~ must satisfy the condition that, for any integer $n$ and non-zero integer $d$, if $x$ is ~(make-rat n d)~, then

$$
\frac{(\text{numer x})}{(\text{denom x})} = \frac{n}{d}
$$

In fact, this is the only condition ~make-rat~, ~numer~, and ~denom~ must fulfill in order to form a suitable basis for a rational-number representation. In general, we can think of data as defined by some collection of selectors and constructors, together with specified conditions that these procedures must fulfill in order to be a valid representations.

This point of view can serve to define not only "high-level" data objects, such as rational-numbers, but lower-level objects as well. Consider the notion of a pair, which we used in order to define our rational numbers. We never actually said what a pair was, only the language supplied procedures ~cons~, ~car~, and ~cdr~ for operating on pairs. But the only thing we need to know about these three operations is that if we glue two objects together using ~cons~ we can retrieve the objects using ~car~ and ~cdr~. That is, the operations satisfy the condition that, for any objects ~x~ and ~y~, if ~z~ is ~(cons x y)~ then ~(car z)~ is ~x~ and ~(cdr z)~ is y. Indeed, we mentioned that these three procedures are included as primitives in our language. However, any triple of procedures that satisfies the above condition can be used as the basis for implementing pairs. This point is illustrate strikingly by the fact that we could implement ~cons~, ~car~, and ~cdr~ without using any data structures at all but only using procedures. Here are the definitions:
#+begin_src scheme
  (define (cons x y)
    (define (dispatch m)
      (cond ((= m 0) x)
            ((= m 1) y)
            (else (error "Argument not 0 or 1 -- CONS" m))))
    dispatch)

  (define (car z) (z 0))
  (define (cdr z) (z 1))
#+end_src

This use of procedures corresponds to nothing like our intuitive notion of what data should be. Nevertheless, all we need to do to show that this is a valid way to represent pairs is to verify that these procedures satisfy the condition given above.

Notice that the ~cons~ procedure actually returns a procedure. And a procedure, and is what ~z~ really is. When we call ~car~, ~z~ is called with formal argument 0, and will return ~x~. The same is for ~cdr~. Therefore, this procedure implementation of pairs is a valid implementation, and if we access pairs using only ~cons~, ~car~, and ~cdr~ we cannot distinguish this implementation from one that uses "real" data structures.

The point of exhibiting the procedural representation of pairs is not that our language works this way (Scheme, and Lisp systems in general, implement pairs directly, for efficiency reasons) but that it could work this way. The procedural representation, although obscure, is perfectly adequate way to represent pairs, since it fulfills the only conditions that pairs need to fulfill. This example also demonstrates that the ability to manipulate procedures as objects automatically provides the ability to represent compound data. This may seem a curiosity now, but procedural representations of data will play a central role in our programming repertoire. This style of programming is often called /message passing/, and we will be using it as a basic tool in chapter 3 when we address the issues of modeling and simulation.
*** Extended Exercise: Interval Arithmetic
Alyssa P.Hacker is designing a system to help people solve engineering problems. One feature she wants to provide in her system is the ability to manipulate inexact quantities (such as measured parameters of physical devices) with known precision, so that when computations are done with such approximate quantities the results will be numbers of known precision.

Electrical engineers will be using Alyssa's system to compute electrical quantities. It is sometimes necessary for them to compute the value of a parallel equivalent resistance $R_p$ of two resistors $R_1$ and $R_2$ using the formula:
$$
R_p = \frac{1}{1/R_1 + 1/R_2}
$$
Resistance values are usually known only up to some tolerance guaranteed by the manufacturer of the resistor. For example, if you buy a resistor labeled "6.8 ohms with 10% tolerance" you can only be sure that the resistor has a resistance between $6.8 - 0.68 = 6.12$ and $6.8 + 0.68 = 7.48$ ohms. Thus, if you have a 6.8-ohm 10% resistor in parallel with a 4.7-ohm 5% resistor, the resistance of the combination can range from about 2.58 ohms (if the two resistors are at the lower bounds) to about 2.97 ohms (if the two resistors are at the upper bounds).

Alyssa's idea is to implement "interval arithmetic" as a set of arithmetic operations for combining "intervals" (objects that represent the range of possible value of an inexact quantity). The result of adding, subtracting, multiplying, or dividing two intervals is itself an interval, representing the range of the result.

Alyssa postulates the existence of an abstract object called an "interval" that has two endpoints: a lower bound and an upper bound. She also presumes that, given the endpoints of an interval, she can construct the interval using the data constructor ~make-interval~. Alyssa first writes a procedure for adding two intervals. She reasons that the minimum value the sum could be is the sum of the two upper bounds.
#+begin_src scheme
  (define (add-interval x y)
    (make-interval (+ (lower-bound x) (lower-bound y))
                   (+ (upper-bound x) (upper-bound y))))
#+end_src

Alyssa also works out the product of two intervals by finding the minimum and the maximum of the products of the bounds and using them as the bounds of the resulting interval. (~min~ and ~max~ are primitives that find the minimum or maximum of any number of arguments.)
#+begin_src scheme
  (define (mul-interval x y)
    (let ((p1 (* (lower-bound x) (lower-bound y)))
          (p2 (* (lower-bound x) (upper-bound y)))
          (p3 (* (upper-bound x) (lower-bound y)))
          (p4 (* (upper-bound x) (upper-bound y))))
      (make-interval (min p1 p2 p3 p4)
                     (max p1 p2 p3 p4))))
#+end_src

To divide two intervals, Alyssa multiplies the first by the reciprocal of the second. Note that the bound of the reciprocal interval are the reciprocal of the upper bound and the reciprocal of the lower bound, in that order.
#+begin_src scheme
  (define (div-interval x y)
    (mul-interval x
                  (make-interval (/ 1.0 (upper-bound y))
                                 (/ 1.0 (lower-bound y)))))
#+end_src

Following contents are in exercises 2.7-2.16.

After debugging her program, Alyssa shows it to a potential user, who complains that her program solves the wrong problem. He wants a program that can deal with numbers represented as a center value and an additive tolerance; for example, he wants to work with intervals such as $3.5 \pm 0.15$ rather than $[3.35, 3.65]$. Alyssa returns to her desk and fixes this problem by supplying an alternate constructor and alternate selectors:
#+begin_src scheme
  (define (make-center-width c w)
    (make-interval (- c w) (+ c w)))

  (define (center i)
    (/ (+ (lower-bound i) (upper-bound i)) 2))

  (define (width i)
    (/ (- (upper-bound i) (lower-bound i)) 2))
#+end_src

Unfortunately, most of Alyssa's users are engineers. Real engineering situations usually involve measurements with only a small uncertainty, measured as the ratio of the width of the interval to the midpoint of the interval. Engineers usually specify percentage tolerances on the parameters of devices, as in the resistor specifications given earlier.

After considerable work, Alyssa P. Hacker delivers her finished system. Several years later, after she has forgotten all about it, she gets a frenzied call from an irate user, Lem E. Tweakit. It seems that Lem has noticed that the formula for parallel resistors can be written in two algebraically equivalent ways:

$$
\frac{R_1 R_2}{R_1 + R_2}
$$

and

$$
\frac{1}{1 / R_1 + 1 / R_2}
$$

He has written the following two programs, each of which computes the parallel-resistors formula differently:
#+begin_src scheme
  (define (par1 r1 r2)
    (div-interval (mul-interval r1 r2)
                  (add-interval r1 r2)))

  (define (par2 r1 r2)
    (let ((one (make-interval 1 1)))
      (div-interval one
                    (add-interval (div-interval one r1)
                                  (div-interval one r2)))))
#+end_src

Lem complains that Alyssa's program gives different answers for the two ways of computing. This is a serious complaint.
** Hierarchical Data and the Closure Property
As we have seen, pairs provide a primitive "glue" that we can use to construct compound data objects. We usually use a box-and-pointer notation. Each object is shown as a /pointer/ to a box. The box for a primitive object contains a representation of the object. For example, the box for a number contains a numeral. The box for a pair is actually a double box, the left part containing (a pointer to) the ~car~ of the pair and the right part containing the ~cdr~.

We have already seen that ~cons~ can be used to combine not only numbers but pairs as well. (You made use of this fact, or should have, in doing exercise 2.2 and 2.3.) As a consequence, *pairs provide a universal building block from which we can construct all sorts of data structures*.

The ability to create pairs whose elements are pairs is the essence of list structure's importance as a representational tool. We refer to this ability as the *closure property* of ~cons~. In general, an operation for combining data objects satisfies the closure property if the results of combining thins with that operation can themselves be combined using the same operation.

The use of the word "closure" here comes from abstract algebra, where a set of elements is said to be closed under an operation if applying the operation to elements in the set produces an element that is again an element of the set. (I included this content in exercise 2.14's solution.) The Lisp community also (unfortunately) uses the word "closure" to describe a totally unrelated concept: A closure is an implementation technique for representing procedures with free variables. We do not use the word "closure" in this second sense in this book.

Closure is the key to power in any means of combination because it permits us to create /hierarchical/ structures--structures made up of parts, which themselves are made up of parts, and so on.

From the outset of chapter 1, we've made essential use of closure in dealing with procedures, because all but the very simplest programs rely on the fact that the elements of a combination can themselves be combinations. In this section, we take up the consequences of closure for compound data. We describe some conventional techniques for using pairs to represent sequences and trees, and we exhibit a graphics language that illustrates closure in a vivid way.

The notion that a means of combination should satisfy closure is a straightforward idea. Unfortunately, the data combiners provided in many popular programming languages do not satisfy closure, or make closure cumbersome to exploit. Pascal and C admit structures whose elements are structures. However, *this requires that the programmer manipulate pointers explicitly, and adhere to the restriction that each field of a structure can contain only elements of a pre-specified form.*

Unlike Lisp, these languages have no built-in general-purpose glue that makes it easy to manipulate compound data in a uniform way. This limitation lies behind Alan Perlis's comment in his foreword to this book: "In Pascal the plethora of declarable data structures induces a specialization within functions that inhibits and penalizes casual cooperation. It is better to have 100 functions operate on one data structure than to have 10 functions operate on 10 data structures."

** Representing Sequences
One of the useful structures we can build with pairs is a /sequence/--an ordered collection of data objects. There are, of course, many ways to represent sequences in terms of pairs. We can construct in a straightforward way, which is making the ~car~ of each pair is the corresponding item in the chain, and the ~cdr~ of the pair is the next pair in the chain. The ~cdr~ of the final pair signals the end of the sequence by pointing to a distinguished value that is not a pair, represented in box-and-pointer diagrams as a diagonal line and in programs as the value of the variable ~nil~. The entire sequence is constructed by nested ~cons~ operations:
#+begin_src scheme
  (cons 1
        (cons 2
              (cons 3
                    (cons 4 '())))) 
;; in the textbook, it used null. But it is not a valid value,
;; it will give the error "unbounded variable: nil".
;; so I used '() instead.
#+end_src

Such sequence of pairs, formed by nested ~cons~ es, is called a /list/, and Scheme provides a primitive called ~list~ to help in constructing lists. The above sequence could be produced by ~(list 1 2 3 4)~. In general
#+begin_src scheme
(list <a1> <a2> <a3> ... <an>)
#+end_src
is equivalent to
#+begin_src scheme
(cons <a1> (cons <a2> (cons <a3> (cons ... (cons <an> '()) ...))))
#+end_src

Lisp systems conventionally print lists by printing the sequence of elements, enclosed in parentheses. Thus, the data object in figure 2.4 is printed as ~(1 2 3 4)~:
#+begin_src scheme
  (define one-through-four (list 1 2 3 4))
  one-through-four
  ;; yields (1 2 3 4)
#+end_src

Be careful not to confuse the expression ~(list 1 2 3 4)~ with the list ~(1 2 3 4)~, which is the result obtained when the expression is evaluated. Attempting to evaluate the expression ~(1 2 3 4)~ will signal an error when the interpreter tries to apply the procedure ~1~ to arguments ~2 3 4~.

The value of ~nil~, used to terminate the chain of pairs, can be thought of as a sequences of no elements, the empty list. The word /nil/ is a contraction of the Latin word /nihil/, which means "nothing".
**** List operations
The use of pairs to represent sequences of elements is accompanied by conventional programming techniques for manipulating lists by successively "~cdr~-ing down" the lists. For example, the procedure ~list-ref~ takes as arguments a list and a number ~n~ and returns the ~n~-th item of the list. It is customary to number the elements of the list beginning with 0. The method for computing ~list-ref~ is the following:
1. For $n=0$, ~list-ref~ should return the ~car~ of the list.
2. Otherwise, ~list-ref~ should return the $(n-1)$ st item of the ~cdr~ of the list.

#+begin_src scheme
  (define (list-ref items n)
    (if (= n 0)
        (car items)
        (list-ref (cdr items) (- n 1))))
#+end_src

Often we ~cdr~ down the whole list. To aid this, Scheme includes a primitive predicate ~null?~, which tests whether its argument is the empty list. The procedure ~length~, which returns the number of items in a list, illustrates this typical pattern of use:
#+begin_src scheme
  ;; recursive
  (define (length items)
    (if (null? items)
        0
        (+ 1 (length (cdr items)))))

  ;; or, an iterative process
  (define (length items)
    (define (length-iter a count)
      (if (null? a)
          count
          (length-iter (cdr a) (+ count 1))))
    (length-iter items 0))
#+end_src

Another conventional programming technique is to "~cons~ up" an answer list while ~cdr~-ing down a list, as in the procedure ~append~, which takes two lists as arguments and combines their elements to make a new list:
#+begin_src scheme
  (append (list 1 2 3 4) (list 5 6 7)) ;; (1 2 3 4 5 6 7)
#+end_src

~append~ is also implemented using a recursive plan. To ~append~ lists ~list1~ and ~list2~, do the following:
#+begin_src scheme
  (define (append list1 list2)
    (if (null? list1)
        list2
        (cons (car list1) (append (cdr list1) list2))))
#+end_src
**** Mapping over lists
One extremely useful operation is to apply some transformation to each element in a list and generate the list of results. For instance, the following procedure scales each number in a list by a given factor:
#+begin_src scheme
  (define (scale-list items factor)
    (if (null? items)
        '()
        (cons (* (car items) factor)
              (scale-list (cdr items) factor))))

(scale-list (list 1 2 3 4 5) 10)
;; (10 20 30 40 50)
#+end_src

We can abstract this general idea and  capture it as a common pattern expressed as a higher-order procedure, just as in section 1.3. The higher-order procedure here is called ~map~. It takes as arguments a procedure of one argument and a list, and returns a list of the results produced by applying the procedure to each element in the list:
#+begin_src scheme
  (define (map proc items)
    (if (null? items)
        '()
        (cons (proc (car items))
              (map proc (cdr items)))))
              
(map abs (list -10 2.5 -11.6 17))           ;; 10 2.5 -11.6 17
(map (lambda (x) (* x x)) (list 1 2 3 4))   ;; 1 4 9 16
#+end_src

Scheme standardly provides a ~map~ procedure that is more general than the one described here. This more general ~map~ takes a procedure of $n$ arguments, together with ~n~ lists, and applies the procedure to all the first elements of the lists, all the second elements of the lists, and so on, returning a list of the results. For example:
#+begin_src scheme
  (map + (list 1 2 3) (list 40 50 60) (list 700 800 900)) ;; (741 852 963)
  (map (lambda (x y) (+ x (* 2 y)))
       (list 1 2 3)
       (list 4 5 6))
  ;; 1 + 2 * 4 = 9
  ;; 2 + 2 * 5 = 12
  ;; 3 + 2 * 6 = 15
#+end_src

Now we can give a new definition of ~scale-list~ in terms of ~map~:
#+begin_src scheme
  (define (scale-list items factor)
    (map (lambda (x) (* x factor))
         items))
#+end_src

~Map~ is an important construct, not only because it captures a common pattern, but because it establishes a higher level of abstraction in dealing with lists. In the original definition of ~scale-list~, the recursive structure of the program draws attention to the element-by-element processing of the list. Defining ~scale-list~ in terms of ~map~ suppresses that level of detail and emphasizes that scaling transforms a list of elements to a list of results.

*The difference between the two definition is not that the computer is performing a different process (it isn't) but that we think about the process differently.*

In effect, ~map~ helps establish an abstraction barrier that isolates *the implementation of procedures that transform lists from the details* of *how the elements of the list are extracted and combined*.

Like the barriers shown in figure 2.1, this abstraction gives us the flexibility to change the low-level details of how sequences are implemented, while preserving the conceptual framework of operations that transform sequences to sequences. Section 2.2.3 expands on this use of sequences as a framework for organizing programs.
*** Hierarchical Structures
The representation of sequences in terms of lists generalizes naturally to represent sequences whose elements may themselves be sequences. For example, we can regard the object ~((1 2) 3 4)~ constructed by
#+begin_src scheme
(cons (list 1 2) (list 3 4))
#+end_src
as a list of three items, the first of which is itself a list, ~(1,2)~. Indeed, this is suggested by the form in which the result is printed by the interpreter.

Another way to think of sequences whose elements are sequences is as /trees/. The elements of the sequence are the branches of the tree, and elements that are themselves sequences are sub-trees.

Recursion is a natural tool for dealing with tree structures, since we can often reduce operations on trees to operations on their branches, which reduce in turn to operations on the branches of the branches, and so on, until we reach the leaves of the tree. As an example, compare the ~length~ procedure of section 2.2.1 with the ~count-leaves~ procedure, which returns the total number of leaves of a tree.
#+begin_src scheme
(define x (cons (list 1 2) (list 3 4)))
(length x) ;; 3, recall that length is cdring down until nil
(count-leaves x) ;; 4
(list x x) ;; (((1 2) 3 4) ((1 2) 3 4))
(length (list x x)) ;; 2
(count-leaves (list x x)) ;; 8
#+end_src

To implement ~count-leaves~, recall the recursive plan for computing ~length~:
- Length of a list ~x~ is 1 plus length of the ~cdr~ of ~x~.
- Length of the empty list is 0

~count-leaves~ is similar. The value for the empty list is the same. But in the reduction step, where we strip off the ~car~ of the list, we must take into account that the ~car~ may itself be a tree whose leaves we need to count. Thus the appropriate reduction step is
- ~count-leaves~ of the empty list is 0
- ~count-leaves~ of a tree ~x~ is ~count-leaves~ of the ~car~ of ~x~ plus ~count-leaves~ of the ~cdr~ of ~x~.
- ~count-leaves~ of a leaf is 1.

To aid in writing recursive procedures on trees, Scheme provides the primitive predicate ~pair?~, which tests whether its argument is a pair. Here is the complete procedure:
#+begin_src scheme
  (define (count-leaves x)
    (cond ((null? x) 0)
          ((not (pair? x)) 1)
          (else (+ (count-leaves (car x))
                   (count-leaves (cdr x))))))
#+end_src
**** Mapping over trees
Just as ~map~ is powerful abstraction for dealing with sequences, ~map~ together with recursion is a powerful abstraction for dealing with trees. For instance, the ~scale-tree~ procedure, analogous to ~scale-list~ of section 2.2.1, takes as arguments a numeric factor and a tree whose leaves are numbers. It returns a tree of the same shape, where each number is multiplied by the factor. The recursive plan for ~scale-tree~ is similar to the one of ~count-leaves~:
#+begin_src scheme
  (define (scale-tree tree factor)
    (cond ((null? tree) '())
          ((not (pair? tree)) (* tree factor))
          (else (cons (scale-tree (car tree) factor)
                      (scale-tree (cdr tree) factor)))))

  (scale-tree (list 1 (list 2 (list 3 4) 5) (list 6 7))
              10)
#+end_src

Another way to implement ~scale-tree~ is to regard the tree as a sequence of sub-trees and us ~map~. We map over the sequence, scaling each sub-tree in turn, and return the list of results. In the base case, where the tree is a leaf, we simply multiply by the factor:
#+begin_src scheme
  (define (scale-tree tree factor)
    (map (lambda (sub-tree)
           (if (pair? sub-tree)
               (scale-tree sub-tree factor)
               (* sub-tree factor)))
         tree))
#+end_src

Many tree operations can be implemented by similar combinations of sequence operations and recursion.
*** Sequences as Conventional Interfaces
In working with compound data, we've stressed how data abstraction permits us to design programs without becoming enmeshed in the details of data representations, and how abstraction preserves for us the flexibility to experiment with alternative representations. In this section, we introduce another powerful design principle for working with data structures--the use of /conventional interfaces/.

In section 1.3 we saw how program abstractions, implemented as higher-order procedures, can capture common patterns in programs that deal with numerical data. Our ability to formulate analogous operations for working with compound data depends crucially on *the style in which we manipulate our data structures*. Consider, for example, the following procedure, analogous to the ~count-leaves~ procedure of section 2.2.2, which takes a tree as argument and computes the sum of the squares of the leaves that are odd:
#+begin_src scheme
  (define (sum-odd-squares tree)
    (cond ((null? tree) 0)
          ((not (pair? tree))
           (if (odd? tree) (square tree) 0))
          (else (+ (sum-odd-squares (car tree))
                   (sum-odd-squares (cdr tree))))))
#+end_src

On the surface, this procedure is very different from the following one, which constructs a list of all the even Fibonacci numbers $Fib(k)$, where $k$ is less than or equal to a given integer $n$:
#+begin_src scheme
  (define (even-fibs n)
    (define (next k)
      (if (> k n)
          '()
          (let ((f (fib k)))
            (if (even? f)
                (cons f (next (+ k 1)))
                (next (+ k 1))))))
    (next 0))
#+end_src

Despite the fact that these two procedures are structurally very different, a more abstract description of the two computations reveals a great deal of similarity.

The first program
1. enumerates the leaves of a tree; (ENUMERATE)
2. filters them, selecting the odd ones; (FILTER)
3. squares each of the selected ones; (MAP) and
4. accumulates the results using ~+~, starting with 0. (ACCUMULATE)

The second program
1. enumerates the integers from 0 to $n$; (ENUMERATE)
2. computes the Fibonacci number for each integer; (MAP)
3. filters them, selecting the even ones; (FILTER) and
4. accumulates the results using ~cons~, starting with the empty list. (ACCUMULATE)

A signal-processing engineer would find it natural to *conceptualize these processes in terms of signals flowing through a cascade of stages*, each of which implements part of the program plan.
+------+-------------------------+----------------------+
| STEP | ~sum-odd-squares~       | ~even-fibs~          |
+------+------------+------------+-----------+----------+
|      | ACTION     |TARGET      |ACTION     |TARGET    |
+------+------------+------------+-----------+----------+
| 1.   |enumerate   |tree leaves |enumerate  |integers  |
+------+------------+------------+-----------+----------+
|2.    |filter      |~odd?~      |map        |~fib~     |
+------+------------+------------+-----------+----------+
|3.    |map         |~square~    |filter     |~even?~   |
+------+------------+------------+-----------+----------+
|4.    |accuulate   |~+~, 0      |accumulate |~cons~,   |
|      |            |            |           |~()~      |
+------+------------+------------+-----------+----------+

In ~sum-odd-squares~, we begin with an /enumerator/, which generates a "signal" consisting of the leaves of a given tree. This signal is passed through a /filter/, which eliminates all but the odd elements. The resulting signal is in turn passed through a ~map~, which is a "transducer" that applies the ~square~ procedure to each element. The output of the map is then fed to an accumulator, which combines the elements using ~+~, starting from an initial 0. The plan for ~even-fibs~ is analogous.

Unfortunately, the two procedure definitions above fail to exhibit this signal-flow structure. For instance, if we examine the ~sum-odd-squares~ procedure, we find that the enumeration if implemented partly by the ~null?~ and ~pair?~ tests and partly by the tree-recursive structure of the procedure. Similarly, the accumulation is found partly in the tests and partly in the addition used in the recursion. In general, there are no distinct parts of either procedure that correspond to the elements in the signal-flow description. Our two procedures decompose the computations in a different way, spreading the enumeration over the program and mingling it with the map, the filter, and the accumulation. *If we could organize our programs to make the signal-flow structure manifest in the procedures we write, this would increase the conceptual clarity of the resulting code*.
**** Sequence Operations
The key to organizing programs so as to more clearly reflect the signal-flow structure is *to concentrate on the "signals" that flow from one stage in the process to the next*. If we represent these signals as lists, then we can use list operations to implement the processing at each of the stages. For instance, we can implement the mapping stages of the signal-flow diagrams using the ~map~ procedure from section 2.2.1:
#+begin_src scheme
  (map square (list 1 2 3 4 5))
  ;; (1 4 9 16 25)
#+end_src

Filtering a sequence to select only those elements that satisfy a given predicate is accomplished by
#+begin_src scheme
  (define (filter predicate sequence)
    (cond ((null? sequence) '())
          ((predicate (car sequence))
           (cons (car sequence)
                 (filter predicate (cdr sequence))))
          (else (filter predicate (cdr sequence)))))
#+end_src

For example
#+begin_src scheme
  (filter odd? (list 1 2 3 4 5))
  ;; (1 3 5)
#+end_src

Accumulations can be implemented by
#+begin_src scheme
  (define (accumulate op initial sequence)
    (if (null? sequence)
        initial
        (op (car sequence)
            (accumulate op initial (cdr sequence)))))

  (accumulate + 0 (list 1 2 3 4 5)) ;; 15
  (accumulate * 1 (list 1 2 3 4 5)) ;; 120
  (accumulate cons '() (list 1 2 3 4 5)) ;; (1 2 3 4 5)
#+end_src

All that remains to implement signal-flow diagrams is to enumerate the sequence of elements to be processed. For ~even-fibs~, we need to generate the sequence of integers in a given range, which we can do as follows:
#+begin_src scheme
  (define (enumerate-interval low high)
    (if (> low high)
        '()
        (cons low (enumerate-interval (+ low 1) high))))

  (enumerate-interval 2 7) ;; 2 3 4 5 6 7
#+end_src

To enumerate the leaves of a tree, we can use
#+begin_src scheme
  (define (enumerate-tree tree)
    (cond ((null? tree) '())
          ((not (pair? tree)) (list tree))
          (else (append (enumerate-tree (car tree))
                        (enumerate-tree (cdr tree))))))

  (enumerate-tree (list 1 (list 2 (list 3 4) 5)))
#+end_src

Now we can reformulate ~sum-odd-squares~ and ~even-fibs~ as in the signal-flow diagrams. For ~sum-odd-squares~, we enumerate the sequences of leaves of the tree, filter this to keep only the odd numbers in the sequence, square each element, and sum the results:
#+begin_src scheme
  (define (sum-odd-squares tree)
    (accumulate +
                0
                (map square
                     (filter odd?
                             (enumerate-tree tree)))))
#+end_src

For ~even-fibs~, we enumerate the integers from 0 to $n$, generate the Fibonacci number for each of these integers, filter the resulting sequence to keep only the even elements, and accumulate the results into a list:
#+begin_src scheme
  (define (even-fibs n)
    (accumulate cons
                '()
                (filter even?
                        (map fib
                             (enumerate-interval 0 n)))))
#+end_src

The value of expressing programs as sequence operations is that this help us make program designs that are modular, that is, designs that are constructed by combining relatively independent pieces. We can encourage modular design by providing a library of standard components together with a conventional interface for connecting the components in flexible ways.

Modular construction is a powerful strategy for controlling complexity in engineering design. In real signal-processing applications, for example, designers regularly build systems by cascading elements selected form standardized families of filters and transducers. Similarly, sequence operations provide a library of standard program elements that we can mix and match. For instance, we can reuse pieces from the ~sum-odd-squares~ and ~even-fibs~ procedure in a program that constructs a list of the squares of the first $n+1$ Fibonacci numbers:
#+begin_src scheme
  (define (list-fib-squares n)
    (accumulate cons
                '()
                (map square
                     (map fib
                          (enumerate-interval 0 n)))))

  (list-fib-squares 10)
  ;;(0 1 1 4 9 25 64 169 441 1156 3025)
#+end_src

We can rearrange the pieces and use them in computing the product of the odd integers in a sequence:
#+begin_src scheme
  (define (product-of-squares-of-odd-elements sequence)
    (accumulate *
                1
                (map square
                     (filter odd? sequence))))

  (product-of-squares-of-odd-elements (list 1 2 3 4 5))
  ;; 225
#+end_src

We can also formulate conventional data-processing applications in terms of sequence operations. Suppose we have a sequence of personnel records and we want to find the salary of the highest-paid programmer. Assume that we have a selector ~salary~ that returns the salary of a record, and a predicate ~programmer?~ that test if a record is for a programmer.
Then we can write:
#+begin_src scheme
  (define (salary-of-highest-paid-programmer records)
    (accumulate max
                0
                (map salary
                     (filter programmer? records))))
#+end_src

These examples give just a hint of the vast range of operations that can be expressed as sequence operations.

One of the reasons for the success of Lisp as a programming language is that lists provide a standard medium for expressing ordered collections so that they can be manipulated using higher-order operations.

Sequences, implemented here as lists, serves as a conventional interface that permits us to combine processing modules. Additionally, when we uniformly represent structures as sequences, we have localized the data-structure dependencies in our programs to a small number of sequence operations. By changing these, we can experiment with alternative representations of sequences, while leaving the overall design of our programs intact. We will exploit this capability in section 3.5, when we generalize the sequence-processing paradigm to admit infinite sequences.
*** Nested Mappings
We can extend the sequence paradigm to include many computations that are commonly expressed using nested loops. Consider this problem: Given a positive integer $n$, find all ordered pairs of distinct positive integers $i$ and $j$, where $1 \leq j < i \leq n$, such that $i+j$ is prime. For example, if $n$ is 6, then the pairs are the following:
| i   | 2 | 3 | 4 | 4 | 5 | 6 |  6 |
| j   | 1 | 2 | 1 | 3 | 2 | 1 |  5 |
|-----+---+---+---+---+---+---+----|
| i+j | 3 | 5 | 5 | 7 | 7 | 7 | 11 |

A natural way to organize this computation is to generate the sequence of all ordered pairs of positive integers less than or equal to $n$, filter to select those pairs whose sum is prime, and then, for each pair $(i, j)$ that passes through the filter, produce the triple $(i, j, i+j)$.

Here is a way to generate the sequence of pairs: For each integer $i \leq n$, enumerate the integers $j < i$, and for each such $i$ and $j$ generate the pair $(i, j)$. In terms of sequence operations, we map along the sequence ~(enumerate-interval 1 n)~. For each $i$ in this sequence, we map along the sequence ~(enumerate-interval 1 (- i 1))~. For each $j$ in the latter sequence, we generate the pair $(list\ i\ j)$. This gives us a sequence of pairs for each $i$. Combining all the sequences for all the $i$  (by accumulating with append) produces the required sequence of pairs:
#+begin_src scheme
  (accumulate append
              '()
              (map (lambda (i)
                     (map (lambda (j) (list i j))
                          (enumerate-interval 1 (- i 1))))
                   (enumerate-interval 1 n)))
#+end_src

The combination of mapping and accumulating with ~append~ is so common in this sort of program that we will isolate it as a separate procedure:
#+begin_src scheme
  (define (flatmap proc seq)
    (accumulate append '() (map proc seq)))
#+end_src

Now filter this sequence of pairs to find those whose sum is prime. The filter predicate is called for each element of the sequence; its argument is a pair and it must extract the integers from the pair. Thus, the predicate to apply to each element in the sequence is
#+begin_src scheme
  (define (prime-sum? pair)
    (prime? (+ (car pair) (cadr pair)))) 
;; cadr is a primitive procedure equivalent to (car (cdr x))
#+end_src

Finally, generate the sequence of results by mapping over the filtered pairs using the following procedure, which constructs a triple consisting of the two elements of the pair along with their sum:
#+begin_src scheme
  (define (make-pair-sum pair)
    (list (car pair) (cadr pair) (+ (car pair) (cadr pair))))
#+end_src

Combining all these steps yields the complete procedure:
#+begin_src scheme
  (define (prime-sum-pairs n)
    (map make-pair-sum
         (filter prime-sum?
                 (flatmap
                  (lambda (i)
                    (map (lambda (j) (list i j))
                         (enumerate-interval 1 (- i 1))))
                  (enumerate-interval 1 n)))))
#+end_src

Nested mappings are also useful for sequences other than those that enumerate intervals. Suppose we wish to generate all the permutations of a set $S$; that is, all the ways of ordering the items in the set. For instance, the permutations of $\{1, 2, 3\}$ are $\{1, 2, 3\}$, $\{1, 3, 2\}$, $\{2, 1, 3\}$, $\{2, 3, 1\}$, $\{3, 1, 2\}$, and $\{3, 2, 1\}$. Here is a plan for generating the permutations of $S$: For each item $x$ in $S$, recursively generate the sequence of permutations of $S-x$, and adjoin $x$ to the front of each one. This yields, for each $x$ in $S$, the sequence of permutations of $S$ that begin with $x$.
Combining these sequences for all $x$ gives all the permutations of $S$:
#+begin_src scheme
  (define (permutations s)
    (if (null? s)
        (list '())
        (flatmap (lambda (x)
                   (map (lambda (p) (cons x p))
                        (permutations (remove x s))))
                 s)))

  ;; what I thought was:
  ;; for each x, find the permutations of S-x
  ;; and for all different slots x can be in,
  ;; insert x into the result.
#+end_src

Notice how this strategy reduces the problem of generating permutations of $S$ to the problem of generating the permutations of sets with fewer elements than $S$. In the terminal case, we work our way down to the empty list, which represents a set of no elements. For this, we generate ~(list '())~, which is a sequence with one item, namely the set with no elements. The ~remove~ procedure used in ~permutations~ returns all items in a given sequence except for a given item. This can be expressed as a simple filter:
#+begin_src scheme
  (define (remove item sequence)
    (filter (lambda (x) (not (= x item)))
            sequence))
#+end_src
